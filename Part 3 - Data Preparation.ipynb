{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f968fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/22 09:12:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949f740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "data = spark.read.csv('Dataset/heart_attack_dataset.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50adcf8",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45dedaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- Systolic: integer (nullable = true)\n",
      " |-- Diastolic : integer (nullable = true)\n",
      " |-- Heart Rate: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = true)\n",
      " |-- Family History (1: Yes): integer (nullable = true)\n",
      " |-- Smoking: integer (nullable = true)\n",
      " |-- Obesity: integer (nullable = true)\n",
      " |-- Alcohol Consumption: integer (nullable = true)\n",
      " |-- Exercise Hours Per Week: double (nullable = true)\n",
      " |-- Diet: string (nullable = true)\n",
      " |-- Previous Heart Problems (1 : Yes): integer (nullable = true)\n",
      " |-- Medication Use: integer (nullable = true)\n",
      " |-- Stress Level: integer (nullable = true)\n",
      " |-- Sedentary Hours Per Day: double (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Triglycerides: integer (nullable = true)\n",
      " |-- Physical Activity Days Per Week: integer (nullable = true)\n",
      " |-- Sleep Hours Per Day: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- Hemisphere: string (nullable = true)\n",
      " |-- Heart Attack Risk (1: Yes): integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop a column\n",
    "column_to_drop = \"Patient ID\"\n",
    "data = data.drop(column_to_drop)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "925c0dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 26\n"
     ]
    }
   ],
   "source": [
    "# Count the number of columns\n",
    "num_columns = len(data.columns)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0cc76f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----+\n",
      "|Heart Attack Risk (1: Yes)|count|\n",
      "+--------------------------+-----+\n",
      "|                         1|  925|\n",
      "|                         0| 1574|\n",
      "+--------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform value counts for the variable \"Heart Attack Risk (1: Yes)\"\n",
    "value_counts = data.groupBy(\"Heart Attack Risk (1: Yes)\").count()\n",
    "\n",
    "# Show the value counts\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c3160",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79433057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Calculate mode of the \"Stress Level\" column\n",
    "mode_value = data.groupBy(\"Stress Level\").count().orderBy(\"count\", ascending=False).first()[0]\n",
    "\n",
    "# Impute missing values with the mode\n",
    "data = data.withColumn(\"Stress Level\", when(data[\"Stress Level\"].isNull(), mode_value).otherwise(data[\"Stress Level\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce392e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "exercise_mean = data.select(mean(\"Exercise Hours Per Week\")).collect()[0][0]\n",
    "sedentary_mean = data.select(mean(\"Sedentary Hours Per Day\")).collect()[0][0]\n",
    "bmi_mean = data.select(mean(\"BMI\")).collect()[0][0]\n",
    "sleep_mean = data.select(mean(\"Sleep Hours Per Day\")).collect()[0][0]\n",
    "\n",
    "# Impute missing values with the mean for the \"age\" column\n",
    "data = data.withColumn(\"Exercise Hours Per Week\", when(data[\"Exercise Hours Per Week\"].isNull(), exercise_mean).otherwise(data[\"Exercise Hours Per Week\"]))\n",
    "data = data.withColumn(\"Sedentary Hours Per Day\", when(data[\"Sedentary Hours Per Day\"].isNull(), sedentary_mean).otherwise(data[\"Sedentary Hours Per Day\"]))\n",
    "data = data.withColumn(\"BMI\", when(data[\"BMI\"].isNull(), bmi_mean).otherwise(data[\"BMI\"]))\n",
    "data = data.withColumn(\"Sleep Hours Per Day\", when(data[\"Sleep Hours Per Day\"].isNull(), sleep_mean).otherwise(data[\"Sleep Hours Per Day\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44a8e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "||\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# List of columns to check for missing values\n",
    "columns = data.columns\n",
    "\n",
    "# Create a DataFrame to store the count of missing values for each column\n",
    "missing_values = data.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in columns])\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_columns = missing_values.columns\n",
    "missing_columns_with_values = [col_name for col_name in missing_columns if missing_values.select(col_name).head()[col_name] > 0]\n",
    "\n",
    "# Create a new DataFrame with columns that have missing values\n",
    "missing_values_filtered = missing_values.select(*missing_columns_with_values)\n",
    "\n",
    "# Show the count of missing values for columns with missing values\n",
    "missing_values_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f6f208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2499\n",
      "Number of columns: 26\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows\n",
    "num_rows1 = data.count()\n",
    "\n",
    "# Count the number of columns\n",
    "num_columns1 = len(data.columns)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of rows: {num_rows1}\")\n",
    "print(f\"Number of columns: {num_columns1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fbcf5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(data[\"Heart Rate\"] != 200)\n",
    "data = data.filter(data[\"Exercise Hours Per Week\"] != 40.546388)\n",
    "data = data.filter(data[\"Stress Level\"] != 20)\n",
    "data = data.filter(data[\"Sleep Hours Per Day\"] != 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce546156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2495\n",
      "Number of columns: 26\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(f\"Number of rows: {data.count()}\")\n",
    "print(f\"Number of columns: {len(data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dd32db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No extreme values found for Age\n",
      "No extreme values found for Cholesterol\n",
      "No extreme values found for Systolic\n",
      "No extreme values found for Diastolic \n",
      "No extreme values found for Heart Rate\n",
      "No extreme values found for Diabetes\n",
      "No extreme values found for Family History (1: Yes)\n",
      "No extreme values found for Obesity\n",
      "No extreme values found for Alcohol Consumption\n",
      "No extreme values found for Exercise Hours Per Week\n",
      "No extreme values found for Previous Heart Problems (1 : Yes)\n",
      "No extreme values found for Medication Use\n",
      "No extreme values found for Stress Level\n",
      "No extreme values found for Sedentary Hours Per Day\n",
      "No extreme values found for Income\n",
      "No extreme values found for BMI\n",
      "No extreme values found for Triglycerides\n",
      "No extreme values found for Physical Activity Days Per Week\n",
      "No extreme values found for Sleep Hours Per Day\n",
      "No extreme values found for Heart Attack Risk (1: Yes)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType, StructType, StructField, StringType\n",
    "\n",
    "# Function to find extreme values for a column\n",
    "\n",
    "def find_extreme_values(data, column):\n",
    "    # Calculate Q1 and Q3\n",
    "    quantiles = data.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "    Q1, Q3 = quantiles[0], quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Filter extreme values\n",
    "    extreme_values_df = data.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "    return extreme_values_df\n",
    "\n",
    "exclude_columns = [\"Smoking\"]\n",
    "\n",
    "# Iterate through each column and find extreme values if the column is numeric\n",
    "for column in data.columns:\n",
    "    if column not in exclude_columns and isinstance(data.schema[column].dataType, (IntegerType, DoubleType)):\n",
    "        extreme_values_df = find_extreme_values(data, column)\n",
    "        if extreme_values_df.count() > 0:\n",
    "            extreme_values = [row[column] for row in extreme_values_df.collect()]\n",
    "            for value in extreme_values:\n",
    "                print(f\"{column} has an extreme value: {value}\")\n",
    "        else:\n",
    "            print(f\"No extreme values found for {column}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5f303",
   "metadata": {},
   "source": [
    "## Construct of new variables / features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17e34314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Create Age Group column based on age bins\n",
    "data = data.withColumn(\n",
    "    \"Age Group\",\n",
    "    when((col(\"Age\") >= 18) & (col(\"Age\") < 30), \"young adults\")\n",
    "    .when((col(\"Age\") >= 30) & (col(\"Age\") < 60), \"middle age\")\n",
    "    .when((col(\"Age\") >= 60) & (col(\"Age\") <= 103), \"old\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15cd210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns: {len(data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58387622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- Systolic: integer (nullable = true)\n",
      " |-- Diastolic : integer (nullable = true)\n",
      " |-- Heart Rate: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = true)\n",
      " |-- Family History (1: Yes): integer (nullable = true)\n",
      " |-- Smoking: integer (nullable = true)\n",
      " |-- Obesity: integer (nullable = true)\n",
      " |-- Alcohol Consumption: integer (nullable = true)\n",
      " |-- Exercise Hours Per Week: double (nullable = true)\n",
      " |-- Diet: string (nullable = true)\n",
      " |-- Previous Heart Problems (1 : Yes): integer (nullable = true)\n",
      " |-- Medication Use: integer (nullable = true)\n",
      " |-- Stress Level: integer (nullable = true)\n",
      " |-- Sedentary Hours Per Day: double (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Triglycerides: integer (nullable = true)\n",
      " |-- Physical Activity Days Per Week: integer (nullable = true)\n",
      " |-- Sleep Hours Per Day: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- Hemisphere: string (nullable = true)\n",
      " |-- Heart Attack Risk (1: Yes): integer (nullable = true)\n",
      " |-- Age Group: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84ac1d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   Age Group|count|\n",
      "+------------+-----+\n",
      "|         old| 1045|\n",
      "|  middle age| 1001|\n",
      "|young adults|  449|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform value counts for the variable \"Heart Attack Risk (1: Yes)\"\n",
    "value_counts = data.groupBy(\"Age Group\").count()\n",
    "\n",
    "# Show the value counts\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "481a5bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the distribution of the \"Country\" variable\n",
    "agegroup_distribution = data.groupBy(\"Age Group\").count()\n",
    "\n",
    "agegroup_distribution_local = agegroup_distribution.toLocalIterator()\n",
    "\n",
    "# Prepare data for plotting\n",
    "agegroup = []\n",
    "counts = []\n",
    "for row in agegroup_distribution_local:\n",
    "    agegroup.append(row[\"Age Group\"])\n",
    "    counts.append(row[\"count\"])\n",
    "\n",
    "# Plot the bar graph\n",
    "plt.bar(agegroup, counts, color='blue')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Age Group')\n",
    "plt.tight_layout()  # Adjust subplots to fit into the figure area\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bd4b5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----+-----------------+\n",
      "|Heart Attack Risk (1: Yes)|count|       percentage|\n",
      "+--------------------------+-----+-----------------+\n",
      "|                         1|  922|36.95390781563127|\n",
      "|                         0| 1573|63.04609218436874|\n",
      "+--------------------------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, lit\n",
    "\n",
    "# Perform value counts for the variable \"Heart Attack Risk (1: Yes)\"\n",
    "total_count = data.count()\n",
    "\n",
    "value_counts = data.groupBy(\"Heart Attack Risk (1: Yes)\").count()\n",
    "value_counts_percentage = value_counts.withColumn(\"percentage\", (col(\"count\") / total_count) * 100)\n",
    "\n",
    "# Show the value counts\n",
    "value_counts_percentage.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a1c5a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----+\n",
      "|Heart Attack Risk (1: Yes)|count|\n",
      "+--------------------------+-----+\n",
      "|                         0|  936|\n",
      "|                         1|  922|\n",
      "+--------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Count the number of samples in each class\n",
    "class_counts = data.groupBy(\"Heart Attack Risk (1: Yes)\").count().collect()\n",
    "count_0 = next(row['count'] for row in class_counts if row['Heart Attack Risk (1: Yes)'] == 0)\n",
    "count_1 = next(row['count'] for row in class_counts if row['Heart Attack Risk (1: Yes)'] == 1)\n",
    "\n",
    "# Determine the minority and majority classes\n",
    "minority_class = 1 if count_1 < count_0 else 0\n",
    "majority_class = 0 if minority_class == 1 else 1\n",
    "minority_count = min(count_0, count_1)\n",
    "\n",
    "# Sample from the majority class\n",
    "majority_class_df = data.filter(col(\"Heart Attack Risk (1: Yes)\") == majority_class)\n",
    "sampled_majority_class_df = majority_class_df.sample(False, minority_count / count_0)\n",
    "\n",
    "# Get all minority class samples\n",
    "minority_class_df = data.filter(col(\"Heart Attack Risk (1: Yes)\") == minority_class)\n",
    "\n",
    "# Combine both classes to get a balanced DataFrame\n",
    "balanced_data = sampled_majority_class_df.union(minority_class_df)\n",
    "\n",
    "# Count the number of samples in each class in the balanced dataset\n",
    "balanced_class_counts = balanced_data.groupBy(\"Heart Attack Risk (1: Yes)\").count()\n",
    "balanced_class_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c34c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the balanced DataFrame to CSV\n",
    "#balanced_data.write.csv(\"balanced_heartdata.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70f3d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "balanced_data = spark.read.csv('Dataset/balanced_heartdata.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c77f56a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- Systolic: integer (nullable = true)\n",
      " |-- Diastolic: integer (nullable = true)\n",
      " |-- Heart Rate: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = true)\n",
      " |-- Family History (1: Yes): integer (nullable = true)\n",
      " |-- Smoking: integer (nullable = true)\n",
      " |-- Obesity: integer (nullable = true)\n",
      " |-- Alcohol Consumption: integer (nullable = true)\n",
      " |-- Exercise Hours Per Week: double (nullable = true)\n",
      " |-- Diet: string (nullable = true)\n",
      " |-- Previous Heart Problems (1 : Yes): integer (nullable = true)\n",
      " |-- Medication Use: integer (nullable = true)\n",
      " |-- Stress Level: integer (nullable = true)\n",
      " |-- Sedentary Hours Per Day: double (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Triglycerides: integer (nullable = true)\n",
      " |-- Physical Activity Days Per Week: integer (nullable = true)\n",
      " |-- Sleep Hours Per Day: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- Hemisphere: string (nullable = true)\n",
      " |-- Heart Attack Risk (1: Yes): integer (nullable = true)\n",
      " |-- Age Group: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "balanced_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26cb7fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1858\n",
      "Number of columns: 27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {balanced_data.count()}\")\n",
    "print(f\"Number of columns: {len(balanced_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ccfc77",
   "metadata": {},
   "source": [
    "## Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cec1aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "file1 = spark.read.csv('Dataset/heart_data_10%(1).csv', header=True, inferSchema=True)\n",
    "file2 = spark.read.csv('Dataset/heart_data_10%(2).csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9137a4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 500\n",
      "Number of columns: 27\n",
      "Number of rows: 500\n",
      "Number of columns: 27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {file1.count()}\")\n",
    "print(f\"Number of columns: {len(file1.columns)}\")\n",
    "\n",
    "print(f\"Number of rows: {file2.count()}\")\n",
    "print(f\"Number of columns: {len(file2.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "143437b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge vertically\n",
    "merge_data = file1.union(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d96b944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1000\n",
      "Number of columns: 27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {merge_data.count()}\")\n",
    "print(f\"Number of columns: {len(merge_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "509c55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the balanced DataFrame to CSV\n",
    "#merge_data.write.csv(\"merge_data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310305f6",
   "metadata": {},
   "source": [
    "##  Reformatting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "509f8c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- Systolic: integer (nullable = true)\n",
      " |-- Diastolic : integer (nullable = true)\n",
      " |-- Heart Rate: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = true)\n",
      " |-- Family History (1: Yes): integer (nullable = true)\n",
      " |-- Smoking: integer (nullable = true)\n",
      " |-- Obesity: integer (nullable = true)\n",
      " |-- Alcohol Consumption: integer (nullable = true)\n",
      " |-- Exercise Hours Per Week: double (nullable = true)\n",
      " |-- Diet: string (nullable = true)\n",
      " |-- Previous Heart Problems (1 : Yes): integer (nullable = true)\n",
      " |-- Medication Use: integer (nullable = true)\n",
      " |-- Stress Level: integer (nullable = true)\n",
      " |-- Sedentary Hours Per Day: double (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Triglycerides: integer (nullable = true)\n",
      " |-- Physical Activity Days Per Week: integer (nullable = true)\n",
      " |-- Sleep Hours Per Day: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- Hemisphere: string (nullable = true)\n",
      " |-- Heart Attack Risk (1: Yes): integer (nullable = true)\n",
      " |-- Age group: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merge_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3ce6e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "||\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# List of columns to check for missing values\n",
    "columns = merge_data.columns\n",
    "\n",
    "# Create a DataFrame to store the count of missing values for each column\n",
    "missing_values = merge_data.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in columns])\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_columns = missing_values.columns\n",
    "missing_columns_with_values = [col_name for col_name in missing_columns if missing_values.select(col_name).head()[col_name] > 0]\n",
    "\n",
    "# Create a new DataFrame with columns that have missing values\n",
    "missing_values_filtered = missing_values.select(*missing_columns_with_values)\n",
    "\n",
    "# Show the count of missing values for columns with missing values\n",
    "missing_values_filtered.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
