{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d322a280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/23 05:27:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbbee9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "data = spark.read.csv('Dataset/merged_data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25875c58",
   "metadata": {},
   "source": [
    "##  Data reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f970d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Encode 'Sex' column\n",
    "data = data.withColumn(\"Sex\", when(data[\"Sex\"] == \"Male\", 0).when(data[\"Sex\"] == \"Female\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d784e8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['Diet', 'Country', 'Continent', 'Hemisphere', 'Age group']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Identify columns with string data type and store them in categorical_columns list\n",
    "categorical_columns = [field.name for field in data.schema.fields if isinstance(field.dataType, StringType)]\n",
    "\n",
    "# Show the list of categorical columns\n",
    "print(\"Categorical columns:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b3b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping for column 'Diet': {'Healthy': 0, 'Unhealthy': 1, 'Average': 2}\n",
      "Mapping for column 'Country': {'Nigeria': 0, 'Germany': 1, 'United States': 2, 'Australia': 3, 'Italy': 4, 'United Kingdom': 5, 'South Africa': 6, 'Argentina': 7, 'Brazil': 8, 'France': 9, 'Colombia': 10, 'Spain': 11, 'Thailand': 12, 'Vietnam': 13, 'China': 14, 'India': 15, 'Japan': 16, 'Canada': 17, 'New Zealand': 18, 'South Korea': 19}\n",
      "Mapping for column 'Continent': {'Asia': 0, 'Europe': 1, 'South America': 2, 'Africa': 3, 'North America': 4, 'Australia': 5}\n",
      "Mapping for column 'Hemisphere': {'Northern Hemisphere': 0, 'Southern Hemisphere': 1}\n",
      "Mapping for column 'Age group': {'Old': 0, 'Middle Age': 1, 'Young Adults': 2}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Dictionary to store mappings\n",
    "mappings = {}\n",
    "\n",
    "# Apply StringIndexer to each categorical column and collect the mappings\n",
    "for column in categorical_columns:\n",
    "    indexer = StringIndexer(inputCol=column, outputCol=f\"{column}_indexed\")\n",
    "    indexer_model = indexer.fit(data)\n",
    "    data = indexer_model.transform(data)\n",
    "    \n",
    "    # Extract and print the mapping of original values to numerical values\n",
    "    labels = indexer_model.labels\n",
    "    mapping = {label: index for index, label in enumerate(labels)}\n",
    "    mappings[column] = mapping\n",
    "    print(f\"Mapping for column '{column}': {mapping}\")\n",
    "    \n",
    "    # Cast the indexed column to integer type\n",
    "    data = data.withColumn(f\"{column}_indexed\", col(f\"{column}_indexed\").cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6261204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: integer (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- Systolic: integer (nullable = true)\n",
      " |-- Diastolic : integer (nullable = true)\n",
      " |-- Heart Rate: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = true)\n",
      " |-- Family History (1: Yes): integer (nullable = true)\n",
      " |-- Smoking: integer (nullable = true)\n",
      " |-- Obesity: integer (nullable = true)\n",
      " |-- Alcohol Consumption: integer (nullable = true)\n",
      " |-- Exercise Hours Per Week: double (nullable = true)\n",
      " |-- Diet: string (nullable = true)\n",
      " |-- Previous Heart Problems (1 : Yes): integer (nullable = true)\n",
      " |-- Medication Use: integer (nullable = true)\n",
      " |-- Stress Level: integer (nullable = true)\n",
      " |-- Sedentary Hours Per Day: double (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Triglycerides: integer (nullable = true)\n",
      " |-- Physical Activity Days Per Week: integer (nullable = true)\n",
      " |-- Sleep Hours Per Day: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- Hemisphere: string (nullable = true)\n",
      " |-- Heart Attack Risk (1: Yes): integer (nullable = true)\n",
      " |-- Age group: string (nullable = true)\n",
      " |-- Diet_indexed: integer (nullable = true)\n",
      " |-- Country_indexed: integer (nullable = true)\n",
      " |-- Continent_indexed: integer (nullable = true)\n",
      " |-- Hemisphere_indexed: integer (nullable = true)\n",
      " |-- Age group_indexed: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3122c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Age group_indexed|count|\n",
      "+-----------------+-----+\n",
      "|                1|  387|\n",
      "|                2|  180|\n",
      "|                0|  433|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform value counts for the variable \"Heart Attack Risk (1: Yes)\"\n",
    "value_counts = data.groupBy(\"Age group_indexed\").count()\n",
    "\n",
    "# Show the value counts\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beba2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original categorical columns\n",
    "data = data.drop(*categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca19563a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: integer (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- Systolic: integer (nullable = true)\n",
      " |-- Diastolic : integer (nullable = true)\n",
      " |-- Heart Rate: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = true)\n",
      " |-- Family History (1: Yes): integer (nullable = true)\n",
      " |-- Smoking: integer (nullable = true)\n",
      " |-- Obesity: integer (nullable = true)\n",
      " |-- Alcohol Consumption: integer (nullable = true)\n",
      " |-- Exercise Hours Per Week: double (nullable = true)\n",
      " |-- Previous Heart Problems (1 : Yes): integer (nullable = true)\n",
      " |-- Medication Use: integer (nullable = true)\n",
      " |-- Stress Level: integer (nullable = true)\n",
      " |-- Sedentary Hours Per Day: double (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Triglycerides: integer (nullable = true)\n",
      " |-- Physical Activity Days Per Week: integer (nullable = true)\n",
      " |-- Sleep Hours Per Day: integer (nullable = true)\n",
      " |-- Heart Attack Risk (1: Yes): integer (nullable = true)\n",
      " |-- Diet_indexed: integer (nullable = true)\n",
      " |-- Country_indexed: integer (nullable = true)\n",
      " |-- Continent_indexed: integer (nullable = true)\n",
      " |-- Hemisphere_indexed: integer (nullable = true)\n",
      " |-- Age group_indexed: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6044a2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns: {len(data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaf1eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target column\n",
    "target_column  = \"Heart Attack Risk (1: Yes)\"\n",
    "\n",
    "# Define the feature columns (excluding the target column)\n",
    "feature_columns  = [col for col in data.columns if col != target_column ]\n",
    "\n",
    "# Split the DataFrame into X (features) and Y (target)\n",
    "X = data.select(*feature_columns)\n",
    "Y = data.select(target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74d41187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Combine feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data_assembled= assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b26c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature                 | Importance\n",
      "-----------------------------------\n",
      "Income                   | 0.10\n",
      "Cholesterol              | 0.10\n",
      "Country_indexed          | 0.09\n",
      "Diastolic                | 0.09\n",
      "Exercise Hours Per Week  | 0.08\n",
      "Heart Rate               | 0.07\n",
      "Triglycerides            | 0.07\n",
      "BMI                      | 0.06\n",
      "Sedentary Hours Per Day  | 0.06\n",
      "Age                      | 0.05\n",
      "Stress Level             | 0.05\n",
      "Systolic                 | 0.03\n"
     ]
    }
   ],
   "source": [
    "# Train a RandomForest model\n",
    "rf = RandomForestClassifier(labelCol=target_column, featuresCol=\"features\", numTrees=10)\n",
    "model = rf.fit(data_assembled)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance = model.featureImportances\n",
    "\n",
    "# Combine feature names and importance scores\n",
    "feature_names = feature_columns\n",
    "feature_importance_dict = dict(zip(feature_names, feature_importance))\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print selected feature names with importance scores\n",
    "#top_n = 12  # Specify the number of top features you want to print\n",
    "#for feature, importance in sorted_features[:top_n]:\n",
    "#    print(f\"Feature: {feature}, Importance: {importance}\")\n",
    "\n",
    "# Print selected feature names with importance scores\n",
    "top_n = 12  # Specify the number of top features you want to print\n",
    "top_features = sorted_features[:top_n]\n",
    "\n",
    "# Print the table header\n",
    "print(\"Feature                 | Importance\")\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "# Print selected feature names with importance scores\n",
    "for feature, importance in top_features:\n",
    "    print(f\"{feature:<24} | {importance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d039287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column names you want to keep\n",
    "columns_to_keep = [\"Heart Attack Risk (1: Yes)\", \"Income\", \"Cholesterol\",\"Country_indexed\", \"Diastolic \",\n",
    "                   \"Exercise Hours Per Week\", \"Heart Rate\", \"Triglycerides\", \"BMI\", \"Sedentary Hours Per Day\",\"Age\",\n",
    "                   \"Stress Level\"]\n",
    "\n",
    "# Select only the columns to keep\n",
    "new_data = data_assembled.select(*columns_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aef14f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Heart Attack Risk (1: Yes): integer (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- Country_indexed: integer (nullable = true)\n",
      " |-- Diastolic : integer (nullable = true)\n",
      " |-- Exercise Hours Per Week: double (nullable = true)\n",
      " |-- Heart Rate: integer (nullable = true)\n",
      " |-- Triglycerides: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Sedentary Hours Per Day: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Stress Level: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc30d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce the DataFrame to a single partition\n",
    "new_data_single_partition = new_data.coalesce(1)\n",
    "\n",
    "# Save the DataFrame to a single CSV file\n",
    "new_data_single_partition.write.csv(\"new_data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "371dd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "new_data = spark.read.csv('Dataset/new_data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38ee7e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Heart Attack Risk (1: Yes): integer (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- Country_indexed: integer (nullable = true)\n",
      " |-- Diastolic: integer (nullable = true)\n",
      " |-- Exercise Hours Per Week: double (nullable = true)\n",
      " |-- Heart Rate: integer (nullable = true)\n",
      " |-- Triglycerides: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Sedentary Hours Per Day: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Stress Level: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72afe995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1000\n",
      "Number of columns: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {new_data.count()}\")\n",
    "print(f\"Number of columns: {len(new_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26b05719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----+\n",
      "|Heart Attack Risk (1: Yes)|count|\n",
      "+--------------------------+-----+\n",
      "|                         1|  509|\n",
      "|                         0|  491|\n",
      "+--------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform value counts for the variable \"Heart Attack Risk (1: Yes)\"\n",
    "value_counts = new_data.groupBy(\"Heart Attack Risk (1: Yes)\").count()\n",
    "\n",
    "# Show the value counts\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fcb0e3",
   "metadata": {},
   "source": [
    "## 4.2 Data Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eedca383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column                         Standard Deviation  \n",
      "-------------------------------------------------------\n",
      "Heart Attack Risk (1: Yes)     | 0.5001691405606395  \n",
      "Income                         | 80973.01257467821   \n",
      "Cholesterol                    | 79.82551652252607   \n",
      "Country_indexed                | 5.749640803508454   \n",
      "Diastolic                      | 14.649868588135842  \n",
      "Exercise Hours Per Week        | 5.735343348205532   \n",
      "Heart Rate                     | 20.165317156345836  \n",
      "Triglycerides                  | 221.82688765586357  \n",
      "BMI                            | 6.298476971781967   \n",
      "Sedentary Hours Per Day        | 3.5290078215488867  \n",
      "Age                            | 21.32262719344007   \n",
      "Stress Level                   | 2.8425418280201344  \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev\n",
    "\n",
    "# Print header\n",
    "print(\"{:<30} {:<20}\".format(\"Column\", \"Standard Deviation\"))\n",
    "print(\"-------------------------------------------------------\")\n",
    "\n",
    "# Calculate and print the standard deviation for each column\n",
    "for column in new_data.columns:\n",
    "    std_dev = new_data.agg(stddev(column)).collect()[0][0]\n",
    "    print(\"{:<30} | {:<20}\".format(column, std_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "650bb175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of log transformed income: 0.6780097873473351\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log\n",
    "\n",
    "# Log transformation of 'income' variable\n",
    "log_income_values = new_data.select(log(new_data[\"Income\"] + 1).alias(\"log_income\"))\n",
    "\n",
    "# Calculate the standard deviation after log transformation\n",
    "std_dev_log_income = log_income_values.agg(stddev(\"log_income\")).collect()[0][0]\n",
    "\n",
    "# Print the standard deviation after log transformation\n",
    "print(\"Standard deviation of log transformed income:\", std_dev_log_income)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
